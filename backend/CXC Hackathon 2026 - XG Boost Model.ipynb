{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/erichuangreal/cxc_hackathon/blob/main/CXC_Hackathon_2026.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvA_3HzGsGIZ",
        "outputId": "6bacb323-1be5-4a91-c3f6-4cf882460758"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost version: 3.1.3\n",
            "\n",
            "Loading data...\n",
            "Dataset shape: (1000, 20)\n",
            "Columns: ['Plot_ID', 'Latitude', 'Longitude', 'DBH', 'Tree_Height', 'Crown_Width_North_South', 'Crown_Width_East_West', 'Slope', 'Elevation', 'Temperature', 'Humidity', 'Soil_TN', 'Soil_TP', 'Soil_AP', 'Soil_AN', 'Menhinick_Index', 'Gleason_Index', 'Disturbance_Level', 'Fire_Risk_Index', 'Health_Status']\n",
            "\n",
            "Target column found: 'Health_Status'\n",
            "\n",
            "Features after removing identifiers: (1000, 12)\n",
            "Features used: ['Slope', 'Elevation', 'Temperature', 'Humidity', 'Soil_TN', 'Soil_TP', 'Soil_AP', 'Soil_AN', 'Menhinick_Index', 'Gleason_Index', 'Disturbance_Level', 'Fire_Risk_Index']\n",
            "\n",
            "Label mapping:\n",
            "  Healthy -> 0\n",
            "  Sub-healthy -> 1\n",
            "  Unhealthy -> 2\n",
            "  Very Healthy -> 3\n",
            "\n",
            "Class distribution:\n",
            "  Healthy: 441 samples (44.1%)\n",
            "  Sub-healthy: 81 samples (8.1%)\n",
            "  Unhealthy: 322 samples (32.2%)\n",
            "  Very Healthy: 156 samples (15.6%)\n",
            "\n",
            "Data split:\n",
            "Training set: (700, 12) (70.0%)\n",
            "Validation set: (150, 12) (15.0%)\n",
            "Test set: (150, 12) (15.0%)\n",
            "\n",
            "==================================================\n",
            "Training XGBoost model...\n",
            "==================================================\n",
            "Training completed!\n",
            "\n",
            "==================================================\n",
            "MODEL PERFORMANCE\n",
            "==================================================\n",
            "Training Accuracy:    1.0000 (100.00%)\n",
            "Validation Accuracy:  0.8133 (81.33%)\n",
            "Test Accuracy:        0.7733 (77.33%)\n",
            "\n",
            "Test Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Healthy       0.75      0.79      0.77        66\n",
            " Sub-healthy       0.50      0.25      0.33        12\n",
            "   Unhealthy       1.00      1.00      1.00        49\n",
            "Very Healthy       0.46      0.52      0.49        23\n",
            "\n",
            "    accuracy                           0.77       150\n",
            "   macro avg       0.68      0.64      0.65       150\n",
            "weighted avg       0.77      0.77      0.77       150\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[52  2  0 12]\n",
            " [ 7  3  0  2]\n",
            " [ 0  0 49  0]\n",
            " [10  1  0 12]]\n",
            "\n",
            "==================================================\n",
            "FEATURE IMPORTANCE\n",
            "==================================================\n",
            "\n",
            "Top 10 Most Important Features:\n",
            " 1. Gleason_Index                  0.3194\n",
            " 2. Disturbance_Level              0.2706\n",
            " 3. Soil_TP                        0.0452\n",
            " 4. Soil_AP                        0.0443\n",
            " 5. Temperature                    0.0438\n",
            " 6. Humidity                       0.0423\n",
            " 7. Soil_TN                        0.0418\n",
            " 8. Fire_Risk_Index                0.0396\n",
            " 9. Menhinick_Index                0.0395\n",
            "10. Slope                          0.0387\n",
            "\n",
            "==================================================\n",
            "EXAMPLE PREDICTION\n",
            "==================================================\n",
            "Required features (12): ['Slope', 'Elevation', 'Temperature', 'Humidity', 'Soil_TN', 'Soil_TP', 'Soil_AP', 'Soil_AN', 'Menhinick_Index', 'Gleason_Index', 'Disturbance_Level', 'Fire_Risk_Index']\n",
            "\n",
            "Example input (median values from training data):\n",
            "  Slope: 22.5580\n",
            "  Elevation: 1508.9079\n",
            "  Temperature: 22.6920\n",
            "  Humidity: 60.1393\n",
            "  Soil_TN: 0.5114\n",
            "  Soil_TP: 0.2498\n",
            "  Soil_AP: 0.2563\n",
            "  Soil_AN: 0.2496\n",
            "  Menhinick_Index: 1.7857\n",
            "  Gleason_Index: 2.9789\n",
            "  Disturbance_Level: 0.5269\n",
            "  Fire_Risk_Index: 0.5298\n",
            "\n",
            "Prediction: Healthy\n",
            "Probabilities:\n",
            "  Healthy: 0.8930\n",
            "  Sub-healthy: 0.1043\n",
            "  Unhealthy: 0.0010\n",
            "  Very Healthy: 0.0016\n",
            "\n",
            "==================================================\n",
            "Model saved as 'forest_health_model.pkl'\n",
            "==================================================\n",
            "\n",
            "Testing saved model load...\n",
            "\n",
            "Test prediction from saved model:\n",
            "  Actual: Unhealthy\n",
            "  Predicted: Unhealthy\n",
            "  Match: True\n",
            "\n",
            "==================================================\n",
            "MODEL SUMMARY\n",
            "==================================================\n",
            "Features used: 12\n",
            "Classes: ['Healthy', 'Sub-healthy', 'Unhealthy', 'Very Healthy']\n",
            "Test accuracy: 77.33%\n",
            "\n",
            "Model is ready for deployment!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Check XGBoost version\n",
        "print(f\"XGBoost version: {xgb.__version__}\")\n",
        "\n",
        "# Load the data\n",
        "print(\"\\nLoading data...\")\n",
        "url = \"https://raw.githubusercontent.com/erichuangreal/cxc_hackathon/refs/heads/main/backend/forest_health_data_with_target.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"\\nTarget column found: 'Health_Status'\")\n",
        "\n",
        "# Prepare features and target\n",
        "# Remove ONLY identifier columns (Plot_ID, Latitude, Longitude)\n",
        "identifier_cols = ['Plot_ID', 'Latitude', 'Longitude', 'DBH', 'Tree_Height', 'Crown_Width_North_South', 'Crown_Width_East_West']\n",
        "X = df.drop(['Health_Status'] + identifier_cols, axis=1)\n",
        "y = df['Health_Status']\n",
        "\n",
        "print(f\"\\nFeatures after removing identifiers: {X.shape}\")\n",
        "print(f\"Features used: {list(X.columns)}\")\n",
        "\n",
        "# Convert target to numerical values\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "print(f\"\\nLabel mapping:\")\n",
        "for i, class_name in enumerate(le.classes_):\n",
        "    print(f\"  {class_name} -> {i}\")\n",
        "\n",
        "# Check class distribution\n",
        "print(f\"\\nClass distribution:\")\n",
        "for class_name in le.classes_:\n",
        "    count = (y == class_name).sum()\n",
        "    percentage = (count / len(y)) * 100\n",
        "    print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "# Split into train, validation, and test sets (70-15-15)\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(f\"\\nData split:\")\n",
        "print(f\"Training set: {X_train.shape} ({X_train.shape[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"Validation set: {X_val.shape} ({X_val.shape[0]/len(df)*100:.1f}%)\")\n",
        "print(f\"Test set: {X_test.shape} ({X_test.shape[0]/len(df)*100:.1f}%)\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build and train XGBoost model\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Training XGBoost model...\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# VERSION 1: For older XGBoost (if you get early_stopping_rounds error)\n",
        "model = XGBClassifier(\n",
        "    n_estimators=200,  # Reduced for faster training\n",
        "    max_depth=4,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    min_child_weight=1,\n",
        "    gamma=0,\n",
        "    reg_alpha=0,\n",
        "    reg_lambda=1,\n",
        "    objective='multi:softprob',\n",
        "    num_class=len(le.classes_),\n",
        "    eval_metric='mlogloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# SIMPLE TRAINING WITHOUT EARLY STOPPING\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Make predictions\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "y_val_pred = model.predict(X_val_scaled)\n",
        "y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Get probabilities\n",
        "y_test_prob = model.predict_proba(X_test_scaled)\n",
        "\n",
        "# Calculate accuracies\n",
        "train_acc = accuracy_score(y_train, y_train_pred)\n",
        "val_acc = accuracy_score(y_val, y_val_pred)\n",
        "test_acc = accuracy_score(y_test, y_test_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Training Accuracy:    {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
        "print(f\"Validation Accuracy:  {val_acc:.4f} ({val_acc*100:.2f}%)\")\n",
        "print(f\"Test Accuracy:        {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
        "\n",
        "print(\"\\nTest Set Classification Report:\")\n",
        "print(classification_report(y_test, y_test_pred, target_names=le.classes_))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "# Feature importance\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"FEATURE IMPORTANCE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get feature importance\n",
        "importance_scores = model.feature_importances_\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X.columns,\n",
        "    'Importance': importance_scores\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "print(\"\\nTop 10 Most Important Features:\")\n",
        "for i, (feature, importance) in enumerate(zip(feature_importance['Feature'][:10],\n",
        "                                              feature_importance['Importance'][:10]), 1):\n",
        "    print(f\"{i:2}. {feature:30} {importance:.4f}\")\n",
        "\n",
        "# Function to make new predictions\n",
        "def predict_health_status(features_dict):\n",
        "    \"\"\"\n",
        "    Predict health status for new data\n",
        "\n",
        "    Parameters:\n",
        "    features_dict: Dictionary with feature names and values\n",
        "    Returns: Dictionary with predicted class and probabilities\n",
        "    \"\"\"\n",
        "    # Create dataframe from input\n",
        "    input_df = pd.DataFrame([features_dict])\n",
        "\n",
        "    # Ensure all columns are in correct order and present\n",
        "    missing_cols = set(X.columns) - set(input_df.columns)\n",
        "    extra_cols = set(input_df.columns) - set(X.columns)\n",
        "\n",
        "    if missing_cols:\n",
        "        raise ValueError(f\"Missing features: {missing_cols}\")\n",
        "    if extra_cols:\n",
        "        print(f\"Warning: Extra features provided: {extra_cols}\")\n",
        "        input_df = input_df[X.columns]  # Keep only needed columns\n",
        "\n",
        "    # Scale the input\n",
        "    input_scaled = scaler.transform(input_df)\n",
        "\n",
        "    # Make prediction\n",
        "    pred_numeric = model.predict(input_scaled)[0]\n",
        "    pred_proba = model.predict_proba(input_scaled)[0]\n",
        "\n",
        "    # Get class name\n",
        "    pred_class = le.inverse_transform([pred_numeric])[0]\n",
        "\n",
        "    # Create result dictionary\n",
        "    result = {\n",
        "        'predicted_class': pred_class,\n",
        "        'predicted_class_numeric': int(pred_numeric),\n",
        "        'probabilities': {le.classes_[i]: float(prob) for i, prob in enumerate(pred_proba)}\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "# Example usage\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"EXAMPLE PREDICTION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = X.columns.tolist()\n",
        "print(f\"Required features ({len(feature_names)}): {feature_names}\")\n",
        "\n",
        "# Create example input using median values from training data\n",
        "example_features = {}\n",
        "for col in feature_names:\n",
        "    example_features[col] = float(X_train[col].median())\n",
        "\n",
        "print(f\"\\nExample input (median values from training data):\")\n",
        "for k, v in example_features.items():\n",
        "    print(f\"  {k}: {v:.4f}\")\n",
        "\n",
        "try:\n",
        "    prediction = predict_health_status(example_features)\n",
        "    print(f\"\\nPrediction: {prediction['predicted_class']}\")\n",
        "    print(f\"Probabilities:\")\n",
        "    for class_name, prob in prediction['probabilities'].items():\n",
        "        print(f\"  {class_name}: {prob:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "\n",
        "# Save the model components\n",
        "import joblib\n",
        "\n",
        "model_components = {\n",
        "    'model': model,\n",
        "    'scaler': scaler,\n",
        "    'label_encoder': le,\n",
        "    'feature_names': feature_names,\n",
        "    'label_mapping': dict(zip(le.classes_, range(len(le.classes_)))),\n",
        "    'class_distribution': dict(zip(le.classes_, np.bincount(y_encoded)))\n",
        "}\n",
        "\n",
        "joblib.dump(model_components, 'forest_health_model.pkl')\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(f\"Model saved as 'forest_health_model.pkl'\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Test loading the model\n",
        "print(\"\\nTesting saved model load...\")\n",
        "loaded_components = joblib.load('forest_health_model.pkl')\n",
        "loaded_model = loaded_components['model']\n",
        "loaded_scaler = loaded_components['scaler']\n",
        "loaded_le = loaded_components['label_encoder']\n",
        "\n",
        "# Make a test prediction\n",
        "test_input_scaled = loaded_scaler.transform(X_test.iloc[:1])\n",
        "test_pred = loaded_model.predict(test_input_scaled)\n",
        "test_pred_class = loaded_le.inverse_transform(test_pred)[0]\n",
        "actual_class = loaded_le.inverse_transform([y_test[0]])[0]\n",
        "\n",
        "print(f\"\\nTest prediction from saved model:\")\n",
        "print(f\"  Actual: {actual_class}\")\n",
        "print(f\"  Predicted: {test_pred_class}\")\n",
        "print(f\"  Match: {actual_class == test_pred_class}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MODEL SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Features used: {len(feature_names)}\")\n",
        "print(f\"Classes: {list(le.classes_)}\")\n",
        "print(f\"Test accuracy: {test_acc*100:.2f}%\")\n",
        "print(f\"\\nModel is ready for deployment!\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPa1x9biIfDUBwRk+5cfy6p",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
